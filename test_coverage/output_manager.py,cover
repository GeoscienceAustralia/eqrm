>> """
>>   Title: output_manager.py
     
>>   Author:  Peter Row, peter.row@ga.gov.au
>>            Duncan Gray, Duncan.gray@ga.gov.au 
   
>>   Description: Load and save data files.
     
>>   Version: $Revision: 965 $  
>>   ModifiedBy: $Author: dgray $
>>   ModifiedDate: $Date: 2009-05-20 15:56:38 +1000 (Wed, 20 May 2009) $
     
>>   Copyright 2007 by Geoscience Australia
>> """
!> import os
!> from gzip import GzipFile
!> from os import listdir
   
!> from scipy import isfinite, array, allclose, asarray, swapaxes, transpose, \
!>      newaxis
   
   
!> EXTENSION = '.txt'
!> FILE_TAG_DELIMITER = '-'
   
!> class myGzipFile(GzipFile):
!>     def __init__(self,name,mode='r'):
!!         GzipFile.__init__(self,name+'.gz',mode)
       
!> def save_hazard(hazard_name,THE_PARAM_T,
!>                 hazard,sites=None,compress=False,
!>                 parallel_tag=None, write_title=True):
!>     """
!>     writes [site_loc]_['soil_SA'|'bedrock_SA']_rp[return period].txt files
   
!>     Gives the latitude and longitude for all sites, as a space seperated file
!>     The first row has comments
   
!>     Additionally, writes several files of SA data.
!>     There is one file per return period.
!>     In these files columns are the period, rows are the location and the
!>     data is SA, in g.
       
!>     """
>>     assert isfinite(hazard).all()
          
>>     base_names = []
>>     if sites is not None:
>>         file_name = save_sites(THE_PARAM_T.savedir, THE_PARAM_T.site_loc,
>>                    sites, compress, parallel_tag, write_title)
>>         base_names.append(file_name)
>>     if compress:
!!         open = myGzipFile
>>     else:
>>         open = file
>>     if parallel_tag is None:
>>         parallel_tag = '' 
>>     for i in range(len(THE_PARAM_T.rtrn_per)):
>>         rp=THE_PARAM_T.rtrn_per[i]
                
>>         base_name = THE_PARAM_T.savedir+THE_PARAM_T.site_loc+ '_'+ \
>>                hazard_name+'_rp' + \
>>                str(rp).replace('.','pt').replace(' ','') + EXTENSION
>>         base_names.append(base_name)
>>         name = base_name + parallel_tag
           
>>         f=open(name,'w')
>>         if write_title:
>>             f.write('% Return period = '+str(rp)+'\n')
>>             f.write(
>>                 '% First row are rsa periods - subsequent rows are sites\n')
>>             f.write(' '.join([str(p) for p in THE_PARAM_T.periods])+'\n')
>>         for j in range(len(hazard)):
>>             hi=hazard[j,:,i] # sites,rsa_per,rtrn            
>>             f.write(' '.join(['%.10g'%(h) for h in hi])+'\n')
>>         f.close()
>>     return base_names
   
!> def load_hazards(saved_dir, site_loc, hazard_name):
!>     """
!>     Load in all of the data written by save hazards.
!>     """
!>     beginning = site_loc+ '_'+ hazard_name+'_rp'
!>     rp_start_index = len(beginning) + 1 # +1 due to the [ bracket.
!>     rp_end_index = -(len(EXTENSION) + 1)
!>     files = listdir(saved_dir)
       #print "files",files 
!>     files = [s for s in files if s.startswith(beginning)and \
!>                s[-4:] == EXTENSION]
!>     if files == []:
!>         raise IOError("No SA files found to load in.")
       #print "files",files
!>     SA_dic = {}
!>     periods = None
!>     for file in files:
!>         number = file[rp_start_index:rp_end_index]
!>         split_num = number.split('pt')
               
!>         rp = file[rp_start_index:rp_end_index].replace('pt','.')
!>         return_period = float(rp) # use as a dic index
           # The conversion has to be done to work on numpy 1.0.4
!>         return_period_array = array([float(rp)],dtype=float) 
           #if len(split_num) == 2:
               #return_period = round(return_period, 
!>         SA_list, periods_f = load_hazard(os.path.join(saved_dir, file))
!>         if periods is None:
!>             periods = periods_f
!>         else:
!>             assert allclose(periods_f, periods)
!>         SA_dic[return_period] = [return_period_array, SA_list]
       # End loop over files/ return periods
       
!>     keys = SA_dic.keys()
!>     keys.sort()
!>     return_p = [SA_dic[key][0] for key in keys]
       #site_len = len(SA_dic[keys[0]][1])
       #period_len = len(SA_dic[keys[0]][1][0])
       # The axis of SA here is return-period, site, period.
       #print "SA_dic", SA_dic
   
       # Swap the axis to site, period, return period
!>     SA = array([SA_dic[key][1] for key in keys])
!>     b = swapaxes(SA, 0, 2)
!>     SA = swapaxes(b, 0, 1)
       
!>     return SA, periods, return_p 
   
!> def load_hazard(file_full_name):
!>     """
!>     Load in one hazard file.
!>     """
!>     f=open(file_full_name,'r')            
!>     text = f.read().splitlines()
       # ditch the comment lines
!>     com = text.pop(0)
!>     com = text.pop(0)
       
!>     periods_f = [float(ix) for ix in text[0].split(' ')]
       
!>     period_line = text.pop(0)
!>     SA_list = []
!>     for line in text:
           # Each line is a site
           #split = line.split(' ')
!>         SA_list.append([float(ix) for ix in line.split(' ')])
!>     return SA_list, periods_f
   
   
!> def load_SA(file_full_name):
!>     """
!>     Given a file in the standard SA format, load it.
!>     Note return periods are ignored.
!>     The SA returned has the axis site, period, return_p
!>     with the return_p axis only having one element.
!>     """
!>     SA_list, periods_f = load_hazard(file_full_name)
       # Modify SA so the axis are correct
!>     SA =array(SA_list)
!>     SA = SA[:,:,newaxis]
       #print "SA",SA
       #print "SA",SA.shape
!>     return SA, periods_f
         
       
!> def save_sites(savedir, site_loc, sites, compress=False,
!>                 parallel_tag=None, write_title=True):
!>     """
!>     Saves Lat and Long info for all sites to a text file.
!>     One row per site.
!>     """
>>     if compress:
!!         open = myGzipFile
>>     else:
>>         open = file
>>     if parallel_tag is None:
>>         parallel_tag = ''
            
>>     base_name =  savedir + site_loc + '_locations.txt'
>>     name = base_name + parallel_tag
>>     loc_file=open(name,'w')
>>     if write_title:
>>         loc_file.write('% latitude, longitude \n')
      
>>     s='\n'.join(['%.8g %.8g'%(lat,lon) for lat,lon in
>>                  zip(sites.latitude,sites.longitude)])        
>>     loc_file.write(s)
>>     loc_file.write('\n')
>>     loc_file.close()
>>     return base_name
   
!> def save_distances(THE_PARAM_T,sites,event_set,compress=False,
!>                 parallel_tag=None):
!>     """
!>     This funtion is called in eqrm analysis.
!>     if THE_PARAM_T.save_motion_flag==1: this is called.
   
!>     This saves two files! break it up into two functions.
!>     """
       
>>     if compress:
!!         open = myGzipFile
>>     else:
>>         open = file
>>     if parallel_tag is None:
>>         parallel_tag = ''
           
>>     dist_mapping = {'Joyner_Boore':'_rjb', 'Rupture':'_rup'}
   
>>     base_names = []
>>     for key in dist_mapping.keys():        
>>         base_name =  THE_PARAM_T.savedir+THE_PARAM_T.site_loc + \
>>                     '_distance' + dist_mapping[key] + '.txt'
>>         base_names.append(base_name)
>>         name = base_name + parallel_tag
>>         dist_file = open(name,'w')
>>         title = '% ' + key + ' distance. Columns are sites, rows are events\n'
>>         dist_file.write(title)
>>         distances_ = sites.distances_from_event_set(event_set).distance(
>>                 key).swapaxes(0,1)
>>         for i in range(len(event_set)):
>>             s='\n'.join([' '.join(['%.5g'%(float(d)) for d in dist]) for dist in [distances_[i]]])
>>             dist_file.write(s)
>>             dist_file.write('\n')            
>>         dist_file.close()  
>>     return base_names   
               
   
!> def save_motion(motion_name,THE_PARAM_T,motion,compress=False,
!>                 parallel_tag=None, write_title=True):
!>     """
!>     Who creates this motion data structure?
!>     How is it defined?
   
!>     There is a file for each event.
!>     First row are rsa periods - subsequent rows are sites.
       
!>     There is a THE_PARAM_T.save_motion_flag.  If it is equal to 1 a
!>     motion file is created.
   
!>     motion_name, such as bedrock or soil
   
!>     """
>>     if compress: open = myGzipFile
>>     else: open = file
>>     if parallel_tag is None:
>>         parallel_tag = ''
>>     base_names = []
>>     for i in range(motion.shape[1]):
           # for all events
>>         base_name =  THE_PARAM_T.savedir+THE_PARAM_T.site_loc+ '_' + \
>>                     motion_name + '_motion_'+str(i)+'.txt'
>>         name = base_name + parallel_tag
>>         base_names.append(base_name)
>>         f=open(name,'w')
>>         if write_title:
>>             f.write('% Event = '+str(i)+'\n')
>>             f.write('% First row are rsa periods - subsequent rows are sites'
>>                     '\n')
>>             f.write(' '.join([str(p) for p in THE_PARAM_T.periods])+'\n')
>>         for j in range(len(motion)):
>>             mi=motion[j,i,:] # sites,event,periods           
>>             f.write(' '.join(['%.10g'%(m) for m in mi])+ '\n')
>>         f.close()
>>     return base_names
   
   
!> def save_structures(THE_PARAM_T,structures,compress=False,
!>                 parallel_tag=None, write_title=True):
!>     """
!>     Save structure information to file.
!>     This funtion is called in eqrm analysis.
!>     """
>>     if compress: open = myGzipFile
>>     else: open = file
>>     if parallel_tag is None:
>!         parallel_tag = ''
>>     base_name =  THE_PARAM_T.savedir+THE_PARAM_T.site_loc + \
>>                 '_structures.txt'
>>     name = base_name + parallel_tag
>>     loc_file=open(name,'w')
>>     if write_title:
>>         loc_file.write('% LATITUDE, LONGITUDE, PRE1989, POSTCODE, SITE_CLASS '+
>>                        'SUBURB SURVEY_FACTOR STRUCTURE_CLASSIFICATION '+
>>                        'HAZUS_STRUCTURE_CLASSIFICATION BID FCB_USAGE ' +
>>                        'HAZUS_USAGE\n')
>>     for i in range(len(structures.latitude)):
>>         loc_file.write('%.6g %.6g %i %i %s %s %.5g %s %s %i %i %s\n'%
>>                        (structures.latitude[i],
>>                         structures.longitude[i],
>>                         structures.attributes['PRE1989'][i],
>>                         structures.attributes['POSTCODE'][i],
>>                         structures.attributes['SITE_CLASS'][i],
>>                         structures.attributes['SUBURB'][i].replace(' ','_'),
>>                         structures.attributes['SURVEY_FACTOR'][i],
>>                         structures.attributes['STRUCTURE_CLASSIFICATION'][i],
>>                         structures.attributes[
>>             'HAZUS_STRUCTURE_CLASSIFICATION'][i],
>>                         structures.attributes['BID'][i],
>>                         structures.attributes['FCB_USAGE'][i],
>>                         structures.attributes['HAZUS_USAGE'][i]))
>>     loc_file.close()
>>     return base_name
   
!> def save_event_set(THE_PARAM_T,event_set,r_new,compress=False):
!>     """
!>     Save event_set information to file.
!>     This funtion is called in eqrm analysis.
!>     """
>>     if compress: open = myGzipFile
>>     else: open = file
   
>>     file_full_name = THE_PARAM_T.savedir + THE_PARAM_T.site_loc + \
>>                      '_event_set.txt'
>>     event_file=open(file_full_name, 'w')    
>>     event_file.write('%column 1: sourcezone index\n') 
>>     event_file.write('%column 2: trace_start_lat\n') 
>>     event_file.write('%column 3: trace_start_lon\n') 
>>     event_file.write('%column 4: trace_end_lat\n') 
>>     event_file.write('%column 5: trace_end_lon\n') 
>>     event_file.write('%column 6: azimuth\n') 
>>     event_file.write('%column 7: dip\n') 
>>     event_file.write('%column 8: Attenuation model index\n') 
>>     event_file.write('%column 9: event activity\n') 
>>     event_file.write('%column 10: Mw\n') 
>>     event_file.write('%column 11: rupture_centroid_lat\n') 
>>     event_file.write('%column 12: rupture_centroid_lon\n') 
>>     event_file.write('%column 13: depth\n') 
>>     event_file.write('%column 14: rupture_x\n') 
>>     event_file.write('%column 15: rupture_y\n') 
>>     event_file.write('%column 16: length\n') 
>>     event_file.write('%column 17: width\n') 
>>     event_file.write('%column 18: Event index\n') 
>>     for i in range(len(event_set)):
>>         s=''
>>         s+=str(event_set.source_zone_id[i])+','
>>         s+=str(event_set.trace_start_lat[i])+','
>>         s+=str(event_set.trace_start_lon[i])+','
>>         s+=str(event_set.trace_end_lat[i])+','
>>         s+=str(event_set.trace_end_lon[i])+','
>>         s+=str(event_set.azimuth[i])+','
>>         s+=str(event_set.dip[i])+','
>>         try:
>>             s+=str(event_set.att_model_index[i])+','
!>         except AttributeError:
!>             s+='-1,'
>>         try:
>>             s+=str(r_new[i])+','
!!         except IndexError:
!!             s+='-1,'
               
>>         s+=str(event_set.Mw[i])+','
>>         s+=str(event_set.rupture_centroid_lat[i])+','
>>         s+=str(event_set.rupture_centroid_lon[i])+','
>>         s+=str(event_set.depth[i])+','
>>         s+=str(event_set.rupture_x[i])+','
>>         s+=str(event_set.rupture_y[i])+','
>>         s+=str(event_set.length[i])+','
>>         s+=str(event_set.width[i])+','
>>         try:
>>             s+=str(event_set.index[i])+'\n'
!>         except AttributeError:
!>             s+=str(i)+'\n'
>>         event_file.write(s)
       #assert len(Mw)==len(r_nu)
       #for i in range(len(Mw)):
       #    mag_file.write('%.5g %.10g\n'%(Mw[i],r_nu[i]))
>>     event_file.close()
>>     return file_full_name # Used in testing
       
!> def load_event_set_subset(saved_dir, site_loc):
!>     """
!>     Load the Mw, and event activity.
!>     """
!>     f=open(os.path.join(saved_dir, site_loc + '_event_set.txt'),'r')
!>     text = f.read().splitlines()
       # ditch the comment lines
!>     for i in range(18):
!>         com = text.pop(0)
           #print "com", com
!>     out = {}
!>     for line in text:
!>         split_line = line.split(',')
!>         out.setdefault('Mw',[]).append(float(split_line[9]))
           # Trying to get 7.6 instead of 7.5999999999999996
           # this did not work
           #out.setdefault('Mw',[]).append(array(split_line[9], dtype=float))
!>         out.setdefault('event_activity',[]).append(float(split_line[8]))
       # Convert to scipy arrays
!>     for k, v in out.items():
!>         v = array(v,dtype=float)
!>     return out      
   
!> def save_damage(save_dir, site_loc, damage_name, damage, building_ids,
!>                  compress=False, parallel_tag=None, write_title=True):
!>     """Save building id and the cumulative probability of the damage to file.
   
!>     save_dir - The directory to save the files into.
!>     site_loc - A string at the beginning of the file name.
!>     damage_name - the type of damage occuring. eg 'structural',
!>                   'non-structural'
!>     damage - A 2D array of cumulative probability of being in a damage state.
!>              Axis site, damage state (4 damage states, slight, moderate,
!>              extensive and complete)
!>     building_id - a 1D array of building ids                  
!>     """
>>     if compress:
!!         open = myGzipFile
>>     else:
>>         open = file
>>     if parallel_tag is None:
>>         parallel_tag = ''  
>>     base_name = os.path.join(save_dir,
>>                              site_loc + "_" + damage_name + '_damage.txt')
>>     name = base_name + parallel_tag
>>     f=open(name,'w')
>>     if write_title is True:
>>         f.write('building_id, slight, moderate, extensive, complete\n')
>>     for building_id, damge_site in map(None, building_ids, damage):
>>         f.write(str(building_id))
>>         f.write(",")
>>         damage_st = ",".join(str(x) for x in damge_site)
>>         f.write(damage_st)
>>         f.write("\n")
>>     f.close()
>>     return base_name
       
       
!> def save_ecloss(ecloss_name,THE_PARAM_T,ecloss,structures,compress=False,
!>                 parallel_tag=None):
!>     """
!>     Save economic loss.
!>     For example; total_building_loss.
       
!>     """
>>     if compress:
!!         open = myGzipFile
>>     else:
>>         open = file
>>     if parallel_tag is None:
>!         parallel_tag = ''  
>>     base_name = os.path.join(THE_PARAM_T.savedir, THE_PARAM_T.site_loc + \
>>                 ecloss_name + '_loss.txt')
>>     name = base_name + parallel_tag
       #print "name", name
>>     f=open(name,'w')
>>     f.write('% First row is bid (building id) - subsequent rows are events\n')
>>     f.write(' '.join([str(bid) for bid in structures.attributes['BID']])
>>                +'\n') 
>>     for i in range(ecloss.shape[1]): # for all events
>>         el=ecloss[:,i] # sites,event
>>         f.write(' '.join(['%.10g'%(l) for l in el])+'\n')
>>     f.close()
>>     return base_name
   
           
!> def save_val(THE_PARAM_T, val, file_tag, compress=False, parallel_tag=None):
>!     if compress:
!!         open = myGzipFile
>!     else:
>!         open = file
>!     if parallel_tag is None:
>!         parallel_tag = ''
>!     base_name =  THE_PARAM_T.savedir+THE_PARAM_T.site_loc + \
>!                  file_tag + '.txt'
>!     name = base_name + parallel_tag
>!     f=open(name,'w')
>!     f.write('\n'.join(['%.10g'%(bv) for bv in val]))
>!     f.write('\n')
>!     f.close()
>!     return base_name
   
       
!> def join_parallel_files(base_names, size, compress=False):
!>     """
!>     Row append a common set of files produced by running EQRM in parallel.
   
!>     The input is a list of base names.
!>     """
!>     if compress: my_open = myGzipFile
!>     else: my_open = open
!>     for base_name in base_names:
           #print "base_name", base_name
!>         f=my_open(base_name,'w')
!>         for i in range(size):
!>             name = base_name + FILE_TAG_DELIMITER + str(i)
!>             f_block =  my_open(name, 'r')
!>             try:
!>                 for line in f_block:
!>                     f.write(line)
!>             finally:
!>                 f_block.close()
!>             os.remove(name)
           
       
!> def join_parallel_files_column(base_names, size, compress=False,
!>                                delimiter=' '):
!>     """
!>     If files are writen where each column represents a structure, combine
!>     the file blocks produced by running in parallel.
!>     NOTE: each row has to end with the delimeter being used.
!>     """
!>     if compress: my_open = myGzipFile
!>     else: my_open = open
!>     for base_name in base_names:
           #print "base_name", base_name
!>         f=my_open(base_name,'w')
!>         f_blocks = []
!>         for i in range(size):       
!>             name = base_name + FILE_TAG_DELIMITER + str(i)
!>             f_blocks.append(my_open(name, 'r'))
!>             comment = f_blocks[i].readline() # Comment from last file.
   
!>         f.write(comment)
!>         while True:
!>             line_blocks = []
!>             for f_block in f_blocks:
!>                 section = f_block.readline()
!>                 line_blocks.append(section[:-1] + delimiter)
!>             line_blocks.append('\n')
!>             if section == '':
!>                 break
!>             row = ''.join(line_blocks)
!>             f.write(row)
!>         f.close()
   
           # Remove all of the block files for this base name.
!>         for i,f_block in enumerate(f_blocks):
!>             f_block.close()
!>             os.remove(base_name + FILE_TAG_DELIMITER + str(i))
   
               
